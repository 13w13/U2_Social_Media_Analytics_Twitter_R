---
title: "U2-Advanced Social Media Analytics"
author: "Edgar Jullien, Antoine Settelen, Simon Weiss"
date: "`r Sys.Date()`"
output:
 html_document:
    toc: true
    toc_float: true
---

# Navigation {.tabset .tabset-fade .tabset-pills}
# Introduction

## Presentation of the project

## 1.1 Load libraries 

```{r message=FALSE, warning=FALSE}
library(ROAuth)
library(RCurl)
library(ggplot2)
library(dplyr)
library(tidytext)
library(maps)
library(rtweet)
library(twitteR)
library(stringr)
library(syuzhet)
library(lubridate)
library(tidyr)
library(tm)
library(stringi)
library(stringr)
library(wordcloud)
library(igraph)
library(sentimentr)
library(tidytext)
library(topicmodels)
library(tidyverse)
library(rvest)
library(reshape2)
```

## 1.2 Load data, transform them into Datatable for better computation time and combine them. 
```{r}
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")

my.key <- "VjKpUNyCAQVnjvgyuPdOrp7Vs"
my.secret <- "LXv3XKqxBMEYSHUKAOAMKH8ASW7fmqrefkjn9CBimVDNO2mzsB"
access_token <- "1321220647624888322-M8mObeu1ANUAi7HOXqfo5ZSk9voOQ7"
access_secret <- "u9El5vMwXMMk32VGxhyQuCDfbe08aagaHUNOIbSCyRDM6"

t <- create_token("FirstApp6934",
                  my.key,
                  my.secret,
                  access_token,
                  access_secret 
                  )

```

```{r}
setup_twitter_oauth(my.key, my.secret, access_token, access_secret)
```
```{r}
#chess <- searchTwitter("chess+the queen's gambit ", n=10000, since='2020-10-23', until='2021-01-24')
```

```{r}
#Chessdata <- search_tweets("CHESS OR NETFLIX OR THE QUEEN'S GAMBIT", n=3000, since="2020-10-23", until = "2021-01-24", token=t)

OmarData <- search_tweets("LUPIN OR OMAR SY", n=100000, type="recent", token=t, lang="en",
                           retryonratelimit = TRUE)
```

```{r}
#Chessdata <- search_tweets("CHESS OR NETFLIX OR THE QUEEN'S GAMBIT", n=3000, since="2020-10-23", until = "2021-01-24", token=t)

#Chessdata <- search_tweets("CHESS OR THE QUEEN'S GAMBIT", n=100000, type = "recent", token=t, language='en',
#                           retryonratelimit = TRUE)
```

```{r}
View(OmarData$text)
```

```{r}
typeof(OmarData)
```

```{r}
length(unique(OmarData$location))
```
```{r}
OmarData$location2<-iconv(OmarData$location,
                            to = "ASCII", sub="")
```

```{r}
OmarData$location2[OmarData$location2==""] <- NA
OmarData$location2[OmarData$location2==", "] <- NA
```

Group City Names

```{r}
OmarData$location2[OmarData$location2 == 'London' & !is.na(OmarData$location2)] = 'London, England'
```

```{r}
View(OmarData$location2[OmarData$location2 == 'London' & !is.na(OmarData$location2)])
```

```{r}
OmarData$location2[OmarData$location2 == 'Paris' & !is.na(OmarData$location2)] = 'Paris, France'
```

```{r}
OmarData$location2[OmarData$location2=='she/her'] <- NA
```

```{r}
OmarData$location2[OmarData$location2 == 'Los Angeles' & !is.na(OmarData$location2)] = 'Los Angeles, CA'
```

```{r}
OmarData$location2[OmarData$location2 == 'New York' & !is.na(OmarData$location2)] = 'New York, NY'
```

```{r}
OmarData$location2[OmarData$location2 == 'Ile-de-France, France' & !is.na(OmarData$location2)] = 'Paris, France'
```

```{r}

OmarData %>%count(location2, sort=TRUE) %>%
  mutate(location2=reorder(location2,n)) %>%
  na.omit()%>% top_n(10)%>%ggplot(aes(x=location2,y=n))+
  geom_bar(stat="identity")+
  coord_flip() +
  labs(x = "Location", y = "Count",
       title = "Most Popular cities - Recent Tweets")+
  theme_light()+
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none")
```
```{r}
ts_plot(OmarData, "hours")+
  ggplot2::theme_minimal()+
  ggplot2::theme(plot.title=ggplot2::element_text(face="bold"))+
  ggplot2::labs(x=NULL,y=NULL,
                title="Frequency",
                subtitle="Twitter status counts 1-hour intervals",
                caption="\nSource: Data collected from Twitter's API"
  )
```

```{r}
OmarData %>%count(source, sort=TRUE) %>%
  mutate(source=reorder(source,n)) %>%
  na.omit()%>% top_n(5)%>%ggplot(aes(x=source,y=n))+
  geom_bar(stat="identity")+
  coord_flip() +
  labs(x = "Location", y = "Count")+
  theme_light()+
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none")
```
Retweet 

```{r}

```







```{r}
View(OmarData)
```





```{r}
usableText <- iconv(OmarData$text, to = "ASCII", sub="")
```

```{r}
OmarData_corpus<-Corpus(VectorSource(usableText))
```

```{r}
View(OmarData_corpus)
```

```{r}
OmarData_corpus<-tm_map(OmarData_corpus,
                          tolower)
OmarData_corpus<-tm_map(OmarData_corpus,
                          removePunctuation)
OmarData_corpus<-tm_map(OmarData_corpus,
                          removeNumbers)
OmarData_corpus<-tm_map(OmarData_corpus,
                          function(x)removeWords(x,
                                                 stopwords("en")))
OmarData_corpus<-tm_map(OmarData_corpus,
                          function(x)removeWords(x,
                                                 stopwords("french")))
OmarData_corpus<-tm_map(OmarData_corpus,
                          function(x)removeWords(x,
                                                 stopwords("italian")))
OmarData_corpus<-tm_map(OmarData_corpus,function(x)removeWords(x,stopwords("spanish")))
```

```{r}
text_corpus <- tm_map(OmarData_corpus,
                      content_transformer(function(x)
                        iconv(x,to='ASCII',sub='byte')))
```
```{r}
# The document-term matrix
OmarData.tdm <- TermDocumentMatrix(text_corpus)
m <- as.matrix(OmarData.tdm)
m[1:5,1:10]
```
```{r}
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 5)
```

```{r}
barplot(d[1:20,]$freq, las = 3,
        names.arg = d[1:20,]$word,col ="lightblue",
        main ="Most frequent words",
        ylab = "Word frequencies")
```
```{r}
findFreqTerms(OmarData.tdm, lowfreq=50)[1:10]
```
```{r}
wordcloud(words = d$word, freq = d$freq, min.freq = 40,
          max.words=100, random.order=FALSE,
          colors=brewer.pal(4,"Dark2"))
```

```{r}
OmarData.tdm<-removeSparseTerms(OmarData.tdm,
                                  sparse=0.95)
```

```{r}
OmarData.df <- as.data.frame(as.matrix(OmarData.tdm))

#View(Chessdata.df)

OmarData.df.scale <- scale(OmarData.df)
```

```{r}
#A word is know seen as a vector 
#We are going to compute the distance between 2 words
#Hierarichical clustering - dendogram  

OmarData.dist <- dist(OmarData.df.scale,
                        method = "euclidean")

OmarData.fit<-hclust(OmarData.dist, method="ward.D2")

plot(OmarData.fit, main="Cluster-Chess") 
```
```{r}
#choose 5 clusters to (hope) have the good numbers of numbers 
#per group 
groups <- cutree(OmarData.fit, k=5) 
plot(OmarData.fit, main="Cluster")
rect.hclust(OmarData.fit, k=5, border="red")
```
```{r}
#we want to know if there is a kind of links within the htags 
#behavior of followers
#relationship between followers tweets 
#a kind of path, a kind of map, a kind of topics

tags<-function(x) toupper(grep("#",strsplit(x,
                                            " +")[[1]],value=TRUE))

l <- nrow(OmarData)
taglist <- vector(mode = "list", l)

texts <- vector(mode = "character", length = l)

```


```{r}
#extract all the tweet paragraphs

for (i in 1:l) texts[i] <- OmarData$text[i]
texts <- iconv(texts, to = "ASCII", sub="")
```

```{r}
# ... and populate it - extract only the # text
j<-0
for(i in 1:l){
  if(is.na(str_match(texts[i],"#"))[1,1]==FALSE){
    j<-j+1
    taglist[[j]]<-str_squish(removePunctuation(tags(ifelse(is.na(str_match(texts[i], "[\n]")[1,1])==TRUE,texts[i],gsub("[\n]"," ",texts[i])))))
  }
}
alltags <- NULL
for (i in 1:l) alltags<-union(alltags,taglist[[i]])
```

```{r}
hash.graph <- graph.empty(directed = T)
# Populate it with nodes
hash.graph <- hash.graph + vertices(alltags)
```

```{r}
for (tags in taglist){
  if (length(tags)>1){ #2 hastags appearing in the same tweet
    for (pair in combn(length(tags),2,simplify=FALSE,
                       FUN=function(x) sort(tags[x]))){
      if (pair[1]!=pair[2]) {
        if (hash.graph[pair[1],pair[2]]==0)
          hash.graph<-hash.graph+edge(pair[1],
                                      pair[2])
      }
    }
  }
}
```


```{r}
V(hash.graph)$color <- "black"
E(hash.graph)$color <- "black"
V(hash.graph)$name <- paste("#",V(hash.graph)$name,
                            sep = "")
V(hash.graph)$label.cex = 0.75
V(hash.graph)$size <- 20
V(hash.graph)$size2 <- 2
hash.graph_simple<-delete.vertices(simplify(hash.graph),
                                   degree(hash.graph)<=5)

plot(hash.graph_simple, edge.width = 2,
     edge.color = "black", vertex.color = "SkyBlue2",
     vertex.frame.color="black", label.color = "black",
     vertex.label.font=2, edge.arrow.size=0.5)
```
```{r}
plain.text<-vector()
for(i in 1:dim(OmarData)[1]){
  plain.text[i]<-OmarData_corpus[[i]][[1]]
}
```

```{r}
sentence_sentiment<-sentiment(get_sentences(plain.text))
sentence_sentiment
```

```{r}

average_sentiment<-mean(sentence_sentiment$sentiment)
average_sentiment
```

```{r}
sd_sentiment<-sd(sentence_sentiment$sentiment)
sd_sentiment
```

```{r}
extract_sentiment_terms(get_sentences(plain.text))
```

```{r}
text_corpus2<-text_corpus[1:200]

doc.lengths<-rowSums(as.matrix(DocumentTermMatrix(text_corpus2)))
```

```{r}
#doc.lengths
#DocumentTermMatrix(text_corpus2)

dtm <- DocumentTermMatrix(text_corpus2[doc.lengths > 0])
```

```{r}
#Latence Diriged Allocation (LDA) #bayses formula ? (not exactly the same) 

# Pick a random seed for replication
SEED = sample(1:1000000, 1)
# Let's start with 2 topics
k = 3
Topics_results<-LDA(dtm, k = k, control = list(seed = SEED))

terms(Topics_results,15)

topics(Topics_results)

tidy_model_beta<-tidy(Topics_results, matrix = "beta")

tidy_model_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  ggplot(aes(reorder(term, beta),beta,fill=factor(topic)))+
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_fill_viridis_d() +
  coord_flip() +
  labs(x = "Topic",
       y = "beta score",
       title = "Topic modeling")
```