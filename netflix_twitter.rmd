---
title: "U2-Advanced Social Media Analytics"
author: "Edgar Jullien, Antoine Settelen, Simon Weiss"
date: "`r Sys.Date()`"
output:
 html_document:
    toc: true
    toc_float: true
---

# Navigation {.tabset .tabset-fade .tabset-pills}
# Introduction

## Presentation of the project

## 1.1 Load libraries 

```{r message=FALSE, warning=FALSE}
library(ROAuth)
library(RCurl)
library(ggplot2)
library(dplyr)
library(tidytext)
library(maps)
library(rtweet)
library(twitteR)
library(stringr)
library(syuzhet)
library(lubridate)
library(tidyr)
library(tm)
library(stringi)
library(stringr)
library(wordcloud)
library(igraph)
library(sentimentr)
library(tidytext)
library(topicmodels)
library(tidyverse)
library(rvest)
library(reshape2)
```

## 1.2 Load data, transform them into Datatable for better computation time and combine them. 
```{r}
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")

my.key <- "VjKpUNyCAQVnjvgyuPdOrp7Vs"
my.secret <- "LXv3XKqxBMEYSHUKAOAMKH8ASW7fmqrefkjn9CBimVDNO2mzsB"
access_token <- "1321220647624888322-M8mObeu1ANUAi7HOXqfo5ZSk9voOQ7"
access_secret <- "u9El5vMwXMMk32VGxhyQuCDfbe08aagaHUNOIbSCyRDM6"

t <- create_token("FirstApp6934",
                  my.key,
                  my.secret,
                  access_token,
                  access_secret 
                  )

```


```{r}
setup_twitter_oauth(my.key, my.secret, access_token, access_secret)
```
```{r}
bigdata <- searchTwitter("#macron", n=150)

bigdata
```

```{r}
#Chessdata <- search_tweets("CHESS OR NETFLIX OR THE QUEEN'S GAMBIT", n=3000, since="2020-10-23", until = "2021-01-24", token=t)

Chessdata <- search_tweets("CHESS OR NETFLIX OR THE QUEEN'S GAMBIT", n=10000, type = "recent", token=t)

View(Chessdata)

```
```{r}
length(unique(Chessdata$location))
```
```{r}
Chessdata$location2<-iconv(Chessdata$location,
                            to = "ASCII", sub="")
```

```{r}
Chessdata$location2[Chessdata$location2==""] <- NA
Chessdata$location2[Chessdata$location2==", "] <- NA
```

```{r}

Chessdata %>%count(location2, sort=TRUE) %>%
  mutate(location2=reorder(location2,n)) %>%
  na.omit()%>% top_n(10)%>%ggplot(aes(x=location2,y=n))+
  geom_bar(stat="identity")+geom_col()+coord_flip() +
  labs(x = "Location", y = "Count",
       title = "Twitter users - unique locations ")+
  theme_light()

```
```{r}
ts_plot(Chessdata, "hours")+
  ggplot2::theme_minimal()+
  ggplot2::theme(plot.title=ggplot2::element_text(face="bold"))+
  ggplot2::labs(x=NULL,y=NULL,
                title="Frequency",
                subtitle="Twitter status counts 1-hour intervals",
                caption="\nSource: Data collected from Twitter's API"
  )

```
```{r}
usableText <- iconv(Chessdata$text, to = "ASCII", sub="")
```
```{r}
Chessdata_corpus<-Corpus(VectorSource(usableText))
```

```{r}
View(Chessdata_corpus)
```

```{r}
Chessdata_corpus<-tm_map(Chessdata_corpus,
                          tolower)
Chessdata_corpus<-tm_map(Chessdata_corpus,
                          removePunctuation)
Chessdata_corpus<-tm_map(Chessdata_corpus,
                          removeNumbers)
Chessdata_corpus<-tm_map(Chessdata_corpus,
                          function(x)removeWords(x,
                                                 stopwords("en")))
Chessdata_corpus<-tm_map(Chessdata_corpus,
                          function(x)removeWords(x,
                                                 stopwords("french")))
Chessdata_corpus<-tm_map(Chessdata_corpus,
                          function(x)removeWords(x,
                                                 stopwords("italian")))
Chessdata_corpus<-tm_map(Chessdata_corpus,function(x)removeWords(x,stopwords("spanish")))
```

```{r}
text_corpus <- tm_map(Chessdata_corpus,
                      content_transformer(function(x)
                        iconv(x,to='ASCII',sub='byte')))
```
```{r}
# The document-term matrix
Chessdata.tdm <- TermDocumentMatrix(text_corpus)
m <- as.matrix(Chessdata.tdm)
m[1:5,1:10]
```
```{r}
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 5)
```

```{r}
barplot(d[1:20,]$freq, las = 3,
        names.arg = d[1:20,]$word,col ="lightblue",
        main ="Most frequent words",
        ylab = "Word frequencies")
```
```{r}
findFreqTerms(Chessdata.tdm, lowfreq=50)[1:10]
```
```{r}
wordcloud(words = d$word, freq = d$freq, min.freq = 40,
          max.words=100, random.order=FALSE,
          colors=brewer.pal(4,"Dark2"))
```

```{r}
Chessdata.tdm<-removeSparseTerms(Chessdata.tdm,
                                  sparse=0.95)
```
```{r}
Chessdata.df <- as.data.frame(as.matrix(Chessdata.tdm))

View(Chessdata.df)

Chessdata.df.scale <- scale(Chessdata.df)
```
```{r}
#A word is know seen as a vector 
#We are going to compute the distance between 2 words
#Hierarichical clustering - dendogram  

Chessdata.dist <- dist(Chessdata.df.scale,
                        method = "euclidean")

Chessdata.fit<-hclust(Chessdata.dist, method="ward.D2")

plot(Chessdata.fit, main="Cluster-Chess") 
```
```{r}
#choose 5 clusters to (hope) have the good numbers of numbers 
#per group 
groups <- cutree(Chessdata.fit, k=5) 
plot(Chessdata.fit, main="Cluster-Chess")
rect.hclust(Chessdata.fit, k=5, border="red")
```
```{r}
#we want to know if there is a kind of links within the htags 
#behavior of followers
#relationship between followers tweets 
#a kind of path, a kind of map, a kind of topics

tags<-function(x) toupper(grep("#",strsplit(x,
                                            " +")[[1]],value=TRUE))

l <- nrow(Chessdata)
taglist <- vector(mode = "list", l)

texts <- vector(mode = "character", length = l)

```
```{r}
#extract all the tweet paragraphs

for (i in 1:l) texts[i] <- Chessdata$text[i]
texts <- iconv(texts, to = "ASCII", sub="")
```
```{r}
# ... and populate it - extract only the # text
j<-0
for(i in 1:l){
  if(is.na(str_match(texts[i],"#"))[1,1]==FALSE){
    j<-j+1
    taglist[[j]]<-str_squish(removePunctuation(tags(ifelse(is.na(str_match(texts[i], "[\n]")[1,1])==TRUE,texts[i],gsub("[\n]"," ",texts[i])))))
  }
}
alltags <- NULL
for (i in 1:l) alltags<-union(alltags,taglist[[i]])
```
```{r}
hash.graph <- graph.empty(directed = T)
# Populate it with nodes
hash.graph <- hash.graph + vertices(alltags)
```

```{r}
for (tags in taglist){
  if (length(tags)>1){ #2 hastags appearing in the same tweet
    for (pair in combn(length(tags),2,simplify=FALSE,
                       FUN=function(x) sort(tags[x]))){
      if (pair[1]!=pair[2]) {
        if (hash.graph[pair[1],pair[2]]==0)
          hash.graph<-hash.graph+edge(pair[1],
                                      pair[2])
      }
    }
  }
}
```


```{r}
V(hash.graph)$color <- "black"
E(hash.graph)$color <- "black"
V(hash.graph)$name <- paste("#",V(hash.graph)$name,
                            sep = "")
V(hash.graph)$label.cex = 0.75
V(hash.graph)$size <- 20
V(hash.graph)$size2 <- 2
hash.graph_simple<-delete.vertices(simplify(hash.graph),
                                   degree(hash.graph)<=5)

plot(hash.graph_simple, edge.width = 2,
     edge.color = "black", vertex.color = "SkyBlue2",
     vertex.frame.color="black", label.color = "black",
     vertex.label.font=2, edge.arrow.size=0.5)
```
```{r}
plain.text<-vector()
for(i in 1:dim(Chessdata)[1]){
  plain.text[i]<-Chessdata_corpus[[i]][[1]]
}
```

```{r}
sentence_sentiment<-sentiment(get_sentences(plain.text))
sentence_sentiment

average_sentiment<-mean(sentence_sentiment$sentiment)
average_sentiment

sd_sentiment<-sd(sentence_sentiment$sentiment)
sd_sentiment
```

```{r}
extract_sentiment_terms(get_sentences(plain.text))
```

```{r}
text_corpus2<-text_corpus[1:200]

doc.lengths<-rowSums(as.matrix(DocumentTermMatrix(text_corpus2)))
```
```{r}
#doc.lengths
#DocumentTermMatrix(text_corpus2)

dtm <- DocumentTermMatrix(text_corpus2[doc.lengths > 0])
```

```{r}
#Latence Diriged Allocation (LDA) #bayses formula ? (not exactly the same) 

# Pick a random seed for replication
SEED = sample(1:1000000, 1)
# Let's start with 2 topics
k = 2
Topics_results<-LDA(dtm, k = k, control = list(seed = SEED))

terms(Topics_results,15)

topics(Topics_results)

tidy_model_beta<-tidy(Topics_results, matrix = "beta")

tidy_model_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  ggplot(aes(reorder(term, beta),beta,fill=factor(topic)))+
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_fill_viridis_d() +
  coord_flip() +
  labs(x = "Topic",
       y = "beta score",
       title = "Topic modeling")
```


















# 2. EDA

## 2.1 Intro : map 