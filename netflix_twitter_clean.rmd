---
title: "U2-Advanced Social Media Analytics"
author: "Edgar Jullien, Antoine Settelen, Simon Weiss"
date: "`r Sys.Date()`"
output:
 html_document:
    toc: true
    toc_float: true
---

# Introduction

## Presentation of the project



## 1.1 Load libraries 

```{r message=FALSE, warning=FALSE}
library(ROAuth)
library(RCurl)
library(ggplot2)
library(dplyr)
library(tidytext)
library(maps)
library(rtweet)
library(twitteR)
library(stringr)
library(syuzhet)
library(lubridate)
library(tidyr)
library(tm)
library(stringi)
library(stringr)
library(wordcloud)
library(igraph)
library(sentimentr)
library(tidytext)
library(topicmodels)
library(tidyverse)
library(rvest)
library(reshape2)
```

## 1.2 Set Token for twitter api and set .pem for encryption 
```{r}
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")

my.key <- "VjKpUNyCAQVnjvgyuPdOrp7Vs"
my.secret <- "LXv3XKqxBMEYSHUKAOAMKH8ASW7fmqrefkjn9CBimVDNO2mzsB"
access_token <- "1321220647624888322-M8mObeu1ANUAi7HOXqfo5ZSk9voOQ7"
access_secret <- "u9El5vMwXMMk32VGxhyQuCDfbe08aagaHUNOIbSCyRDM6"

t <- create_token("FirstApp6934",
                  my.key,
                  my.secret,
                  access_token,
                  access_secret 
                  )

```

```{r}
setup_twitter_oauth(my.key, my.secret, access_token, access_secret)
```

Donwload tweets with "Lupin" or "Omar Sy" in English. 
```{r}

OmarData <- search_tweets("LUPIN OR OMAR SY", n=100000, type="recent", token=t, lang="en",
                           retryonratelimit = TRUE)
```


```{r}
View(OmarData$text)
```
```{r}
length(unique(OmarData))
```
Without retweets, he have 1371 tweets about this topic within the week. 

```{r}
typeof(OmarData)
```
Without retweet, we have


## 2. Study the location of the tweets

```{r}
length(unique(OmarData$location))
```
We have 1225 different location of the tweets



### 2.1 Cleaning steps


We convert the location into ASCII format

```{r}
OmarData$location2<-iconv(OmarData$location,
                            to = "ASCII", sub="")
```

Then we do some cleaning

```{r}
OmarData$location2[OmarData$location2==""] <- NA
OmarData$location2[OmarData$location2==", "] <- NA
OmarData$location2[OmarData$location2 == 'London' & !is.na(OmarData$location2)] = 'London, England'
OmarData$location2[OmarData$location2 == 'Paris' & !is.na(OmarData$location2)] = 'Paris, France'
OmarData$location2[OmarData$location2=='she/her'] <- NA
OmarData$location2[OmarData$location2 == 'Los Angeles' & !is.na(OmarData$location2)] = 'Los Angeles, CA'
OmarData$location2[OmarData$location2 == 'New York' & !is.na(OmarData$location2)] = 'New York, NY'
OmarData$location2[OmarData$location2 == 'Ile-de-France, France' & !is.na(OmarData$location2)] = 'Paris, France'

```

### 2.2 Plot Cities

```{r}

OmarData %>%count(location2, sort=TRUE) %>%
  mutate(location2=reorder(location2,n)) %>%
  na.omit()%>% top_n(10)%>%ggplot(aes(x=location2,y=n))+
  geom_bar(stat="identity")+
  coord_flip() +
  labs(x = "Location", y = "Count",
       title = "Most Popular cities - Recent Tweets")+
  theme_light()+
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none")
```
We can observe that the French tv shows has a wide international audience. Its first public comes from England, London then Pars and Los Angeles. This is a great indicator of the international success of a french serie.  


## 3. Analyse the frenquency of the tweets

### 3.1 Plot the time series

```{r}
ts_plot(OmarData, "hours")+
  ggplot2::theme_minimal()+
  ggplot2::theme(plot.title=ggplot2::element_text(face="bold"))+
  ggplot2::labs(x=NULL,y=NULL,
                title="Frequency",
                subtitle="Twitter status counts 1-hour intervals",
                caption="\nSource: Data collected from Twitter's API"
  )
```

### 4. Study the device used for tweeting
```{r}
OmarData %>%count(source, sort=TRUE) %>%
  mutate(source=reorder(source,n)) %>%
  na.omit()%>% top_n(5)%>%ggplot(aes(x=source,y=n))+
  geom_bar(stat="identity")+
  coord_flip() +
  labs(x = "Location", y = "Count")+
  theme_light()+
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "none")
```
We can analyse that most of the tweets come from mobile device : first Iphone user and then Android. The most of the rest come from the desktop web app. 

##Retweet 

```{r}
retweet_table <- as.data.frame(prop.table(table(OmarData$is_retweet)))
bp<- ggplot(retweet_table, aes(x="", y=Freq, fill=Var1))+
geom_bar(width = 1, stat = "identity")
retweet_pie <- bp + coord_polar("y", start=0)
retweet_pie + scale_fill_brewer("Blues") +
  theme(axis.text.x=element_blank()) +
   geom_text(aes(y = Freq/2 + c(0, cumsum(Freq)[-length(Freq)]), 
                label =round(Freq,2) ), size=5) +
  labs(x=NULL,y=NULL,
                title="Retweet ratio",
                caption="\nSource: Data collected from Twitter's API") +
  theme(plot.title = element_text(hjust = 0.5))
```

##Hastags

```{r}
hastags <- as.data.frame(table(unlist(OmarData$hashtags)))
wordcloud(words = hastags$Var1, freq = hastags$Freq, min.freq = 2,
          max.words=100, random.order=FALSE, scale = c(2,2),
          colors=brewer.pal(3,"Dark2"))
```
##Mention

```{r}
mentions <- as.data.frame(table(unlist(OmarData$mentions_screen_name)))
wordcloud(words = mentions$Var1, freq = mentions$Freq, min.freq = 5,
          max.words=100, random.order=FALSE,
          colors=brewer.pal(3,"Dark2"))
```

##Verified
```{r}
verified_table <- as.data.frame(prop.table(table(OmarData$verified)))
bp<- ggplot(verified_table, aes(x="", y=Freq, fill=Var1))+
geom_bar(width = 1, stat = "identity")
verified_pie <- bp + coord_polar("y", start=0)
verified_pie + scale_fill_brewer("Blues") +
  theme(axis.text.x=element_blank()) +
   geom_text(aes(y = Freq/2 + c(0, cumsum(Freq)[-length(Freq)]), 
                label =round(Freq,2) ), size=5)  +
  labs(x=NULL,y=NULL,
                title="Verified ratio",
                caption="\nSource: Data collected from Twitter's API") +
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
usableText <- iconv(OmarData$text, to = "ASCII", sub="")
```

```{r}
OmarData_corpus<-Corpus(VectorSource(usableText))
```

```{r}
#View(OmarData_corpus)
```

```{r}
OmarData_corpus<-tm_map(OmarData_corpus,
                          tolower)
OmarData_corpus<-tm_map(OmarData_corpus,
                          removePunctuation)
OmarData_corpus<-tm_map(OmarData_corpus,
                          removeNumbers)
OmarData_corpus<-tm_map(OmarData_corpus,
                          function(x)removeWords(x,
                                                 stopwords("en")))
OmarData_corpus<-tm_map(OmarData_corpus,
                          function(x)removeWords(x,
                                                 stopwords("french")))
OmarData_corpus<-tm_map(OmarData_corpus,
                          function(x)removeWords(x,
                                                 stopwords("italian")))
OmarData_corpus<-tm_map(OmarData_corpus,function(x)removeWords(x,stopwords("spanish")))
```

```{r}
text_corpus <- tm_map(OmarData_corpus,
                      content_transformer(function(x)
                        iconv(x,to='ASCII',sub='byte')))
```
```{r}
# The document-term matrix
OmarData.tdm <- TermDocumentMatrix(text_corpus)
m <- as.matrix(OmarData.tdm)
m[1:5,1:10]
```
```{r}
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 5)
```

```{r}
barplot(d[1:20,]$freq, las = 3,
        names.arg = d[1:20,]$word,col ="lightblue",
        main ="Most frequent words",
        ylab = "Word frequencies")
```
```{r}
findFreqTerms(OmarData.tdm, lowfreq=50)[1:10]
```
```{r}
wordcloud(words = d$word, freq = d$freq, min.freq = 50,
          max.words=100, random.order=FALSE,
          colors=brewer.pal(4,"Dark2"))
```

```{r}
OmarData.tdm<-removeSparseTerms(OmarData.tdm,
                                  sparse=0.95)
```

```{r}
OmarData.df <- as.data.frame(as.matrix(OmarData.tdm))
#View(Chessdata.df)
OmarData.df.scale <- scale(OmarData.df)
```

```{r}
#A word is know seen as a vector 
#We are going to compute the distance between 2 words
#Hierarichical clustering - dendogram  
OmarData.dist <- dist(OmarData.df.scale,
                        method = "euclidean")
OmarData.fit<-hclust(OmarData.dist, method="ward.D2")
plot(OmarData.fit, main="Cluster-Chess") 
```
```{r}
#choose 5 clusters to (hope) have the good numbers of numbers 
#per group 
groups <- cutree(OmarData.fit, k=4) 
plot(OmarData.fit, main="Cluster")
rect.hclust(OmarData.fit, k=4, border="blue")
```
```{r}
#we want to know if there is a kind of links within the htags 
#behavior of followers
#relationship between followers tweets 
#a kind of path, a kind of map, a kind of topics
tags<-function(x) toupper(grep("#",strsplit(x,
                                            " +")[[1]],value=TRUE))
l <- nrow(OmarData)
taglist <- vector(mode = "list", l)
texts <- vector(mode = "character", length = l)
```


```{r}
#extract all the tweet paragraphs
for (i in 1:l) texts[i] <- OmarData$text[i]
texts <- iconv(texts, to = "ASCII", sub="")
```

```{r}
# ... and populate it - extract only the # text
j<-0
for(i in 1:l){
  if(is.na(str_match(texts[i],"#"))[1,1]==FALSE){
    j<-j+1
    taglist[[j]]<-str_squish(removePunctuation(tags(ifelse(is.na(str_match(texts[i], "[\n]")[1,1])==TRUE,texts[i],gsub("[\n]"," ",texts[i])))))
  }
}
alltags <- NULL
for (i in 1:l) alltags<-union(alltags,taglist[[i]])
```


```{r}
hash.graph <- graph.empty(directed = T)
# Populate it with nodes
hash.graph <- hash.graph + vertices(alltags)
```

```{r}
for (tags in taglist){
  if (length(tags)>1){ #2 hastags appearing in the same tweet
    for (pair in combn(length(tags),2,simplify=FALSE,
                       FUN=function(x) sort(tags[x]))){
      if (pair[1]!=pair[2]) {
        if (hash.graph[pair[1],pair[2]]==0)
          hash.graph<-hash.graph+edge(pair[1],
                                      pair[2])
      }
    }
  }
}
```


```{r}
V(hash.graph)$color <- "black"
E(hash.graph)$color <- "black"
V(hash.graph)$label.cex = 0.75
V(hash.graph)$size <- 50
V(hash.graph)$size2 <- 4
hash.graph_simple<-delete.vertices(simplify(hash.graph),
                                   degree(hash.graph)<= 9)
plot(hash.graph_simple, edge.width = 2,
     edge.color = "black", vertex.color = "SkyBlue2",
     vertex.frame.color="black", label.color = "black",
     vertex.label.font=2, edge.arrow.size=0.5)
```
```{r}
plain.text<-vector()
for(i in 1:dim(OmarData)[1]){
  plain.text[i]<-OmarData_corpus[[i]][[1]]
}
```

```{r}
sentence_sentiment<-sentiment(get_sentences(plain.text))
sentence_sentiment
```

##Mean sentiment analysis
```{r}
average_sentiment<-mean(sentence_sentiment$sentiment)
average_sentiment
```

##Median sentiment analysis
```{r}
sd_sentiment<-sd(sentence_sentiment$sentiment)
sd_sentiment
```

```{r}
extract_sentiment_terms(get_sentences(plain.text))
```

## Negatives words
```{r}
negative_words <- as.data.frame(table(unlist(extract_sentiment_terms(get_sentences(plain.text))$negative)))
negative_words$Var1[negative_words$Var1 == 'black'] = NA
negative_words = na.omit(negative_words)
wordcloud(words = negative_words$Var1, freq = negative_words$Freq, min.freq = 4,
          max.words=100, random.order=FALSE,
          colors=brewer.pal(4,"Dark2"))
```
```{r}
negative_words <- negative_words[order(negative_words$Freq, decreasing = TRUE),]
```

```{r}
barplot(negative_words[1:20,]$Freq, las = 3,
        names.arg = negative_words[1:20,]$Var1,col ="lightblue",
        main ="Most frequent negative words",
        ylab = "Word frequencies")
```

```{r}
positive_words <- as.data.frame(table(unlist(extract_sentiment_terms(get_sentences(plain.text))$positive)))
wordcloud(words = positive_words$Var1, freq = positive_words$Freq, min.freq = 4,
          max.words=100, random.order=FALSE,
          colors=brewer.pal(4,"Dark2"))
```

```{r}
positive_words <- positive_words[order(positive_words$Freq, decreasing = TRUE),]
barplot(positive_words[1:20,]$Freq, las = 3,
        names.arg = positive_words[1:20,]$Var1,col ="lightblue",
        main ="Most frequent positive words",
        ylab = "Word frequencies")
```


```{r}
text_corpus2<-text_corpus[1:200]
doc.lengths<-rowSums(as.matrix(DocumentTermMatrix(text_corpus2)))
```

```{r}
#doc.lengths
#DocumentTermMatrix(text_corpus2)
dtm <- DocumentTermMatrix(text_corpus2[doc.lengths > 0])
```

```{r}
#Latence Diriged Allocation (LDA) #bayses formula ? (not exactly the same) 
# Pick a random seed for replication
SEED = sample(1:1000000, 1)
# Let's start with 2 topics
k = 3
Topics_results<-LDA(dtm, k = k, control = list(seed = SEED))
terms(Topics_results,15)
topics(Topics_results)
tidy_model_beta<-tidy(Topics_results, matrix = "beta")
tidy_model_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  ggplot(aes(reorder(term, beta),beta,fill=factor(topic)))+
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_fill_viridis_d() +
  coord_flip() +
  labs(x = "Topic",
       y = "beta score",
       title = "Topic modeling")
```